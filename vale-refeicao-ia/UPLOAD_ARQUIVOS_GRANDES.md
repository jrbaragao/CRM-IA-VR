# ğŸ“ Upload de Arquivos Grandes - SoluÃ§Ãµes

## âš ï¸ LimitaÃ§Ã£o do Cloud Run

O **Google Cloud Run** tem um limite fixo de **32MB** por requisiÃ§Ã£o HTTP. Isso significa que arquivos maiores que ~30MB nÃ£o podem ser uploadados diretamente pela interface web.

## ğŸš€ SoluÃ§Ãµes para Arquivos Grandes

### 1. **ğŸ’» Rodar Localmente (Recomendado)**

Para arquivos de atÃ© **200MB**:

```bash
# Clone o repositÃ³rio
git clone https://github.com/jrbaragao/CRM-IA-VR.git
cd CRM-IA-VR/vale-refeicao-ia

# Configure a chave da OpenAI
echo "OPENAI_API_KEY=sk-sua-chave" > .env
echo "DATABASE_URL=sqlite:///./vale_refeicao.db" >> .env

# Instale as dependÃªncias
pip install -r requirements.txt

# Execute localmente
streamlit run app.py
```

**Vantagens:**
- âœ… Suporte a arquivos atÃ© 200MB
- âœ… Processamento mais rÃ¡pido
- âœ… Dados ficam no seu computador
- âœ… Sem custos de nuvem

### 2. **ğŸ“‚ Dividir Arquivos em Partes**

Se seu arquivo Ã© muito grande, divida-o:

#### **Excel/CSV:**
```python
import pandas as pd

# Ler arquivo grande
df = pd.read_csv('arquivo_grande.csv')

# Dividir em partes de 25MB (aproximadamente 100k linhas)
chunk_size = 100000
for i, chunk in enumerate(df.groupby(df.index // chunk_size)):
    chunk[1].to_csv(f'arquivo_parte_{i+1}.csv', index=False)
```

#### **Via PowerShell (Windows):**
```powershell
# Dividir CSV em partes (por nÃºmero de linhas)
$lines = Get-Content arquivo_grande.csv
$chunkSize = 100000
$fileNumber = 1

for ($i = 0; $i -lt $lines.Count; $i += $chunkSize) {
    $end = [Math]::Min($i + $chunkSize - 1, $lines.Count - 1)
    $lines[$i..$end] | Out-File "arquivo_parte_$fileNumber.csv" -Encoding UTF8
    $fileNumber++
}
```

### 3. **â˜ï¸ Google Drive + Colab (Alternativa)**

Para anÃ¡lises complexas com arquivos muito grandes:

1. **Upload para Google Drive**
2. **Abra Google Colab**
3. **Clone o repositÃ³rio no Colab**:
```python
!git clone https://github.com/jrbaragao/CRM-IA-VR.git
%cd CRM-IA-VR/vale-refeicao-ia

# Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Instalar dependÃªncias
!pip install -r requirements.txt

# Configurar OpenAI
import os
os.environ['OPENAI_API_KEY'] = 'sk-sua-chave'

# Importar e usar as funÃ§Ãµes
from src.agents.calculation_agent import CalculationAgent
```

### 4. **ğŸ”§ Upload Direto para Cloud Storage (AvanÃ§ado)**

Para desenvolvedores que querem manter tudo na nuvem:

```bash
# 1. Upload manual para o bucket
gsutil cp arquivo_grande.csv gs://crmia-uploads-files/uploads/

# 2. Modificar cÃ³digo para ler diretamente do GCS
# (Requer alteraÃ§Ã£o no cÃ³digo da aplicaÃ§Ã£o)
```

## ğŸ“Š Limites por Ambiente

| Ambiente | Limite de Upload | Limite de Processamento | RecomendaÃ§Ã£o |
|----------|------------------|-------------------------|--------------|
| **Cloud Run** | 30MB | 2GB RAM | Arquivos pequenos/mÃ©dios |
| **Local** | 200MB | Conforme seu PC | Arquivos grandes |
| **Google Colab** | 25GB | 12GB RAM | AnÃ¡lises complexas |

## ğŸ¯ Qual Escolher?

### **Arquivo atÃ© 30MB:**
- âœ… Use a versÃ£o Cloud Run (mais conveniente)

### **Arquivo 30MB - 200MB:**
- âœ… Use a versÃ£o local (melhor experiÃªncia)

### **Arquivo > 200MB:**
- âœ… Divida em partes menores
- âœ… Ou use Google Colab para anÃ¡lises pontuais

## ğŸ”„ ImplementaÃ§Ã£o Futura

Estamos trabalhando em:
- **Upload direto para Cloud Storage** (bypass do limite HTTP)
- **Processamento em chunks** (streaming de arquivos)
- **Interface para arquivos muito grandes**

## ğŸ“ Suporte

Para casos especÃ­ficos de arquivos muito grandes ou integraÃ§Ã£o empresarial, entre em contato para soluÃ§Ãµes customizadas.

---

**Dica**: Para uso regular, recomendamos a **versÃ£o local** para arquivos grandes e a **versÃ£o Cloud** para acesso rÃ¡pido e compartilhamento.
